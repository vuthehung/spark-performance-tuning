{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/15 13:36:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders_file = \"../data/bucketing/orders.csv\"\n",
    "df_orders = spark.read.csv(orders_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+--------+----------+------------+\n",
      "|order_id|product_id|customer_id|quantity|order_date|total_amount|\n",
      "+--------+----------+-----------+--------+----------+------------+\n",
      "|1       |80        |10         |4       |2023-3-20 |1003        |\n",
      "|2       |69        |30         |3       |2023-12-11|780         |\n",
      "|3       |61        |20         |4       |2023-4-26 |1218        |\n",
      "|4       |62        |44         |3       |2023-8-26 |2022        |\n",
      "|5       |78        |46         |4       |2023-8-5  |1291        |\n",
      "+--------+----------+-----------+--------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- total_amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders.show(5, False)\n",
    "df_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_file = \"../data/bucketing/products.csv\"\n",
    "df_products = spark.read.csv(products_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-------+-----+-----+\n",
      "|product_id|product_name|category   |brand  |price|stock|\n",
      "+----------+------------+-----------+-------+-----+-----+\n",
      "|1         |Product_1   |Electronics|Brand_4|26   |505  |\n",
      "|2         |Product_2   |Apparel    |Brand_4|489  |15   |\n",
      "|3         |Product_3   |Apparel    |Brand_4|102  |370  |\n",
      "|4         |Product_4   |Groceries  |Brand_1|47   |433  |\n",
      "|5         |Product_5   |Groceries  |Brand_3|244  |902  |\n",
      "+----------+------------+-----------+-------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- stock: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products.show(5, False)\n",
    "df_products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products.select(\"product_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders.select(\"order_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing In Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_product_details = (\n",
    "    df_orders.join(\n",
    "        df_products,\n",
    "        on=\"product_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [product_id#17, order_id#16, customer_id#18, quantity#19, order_date#20, total_amount#21, product_name#76, category#77, brand#78, price#79, stock#80]\n",
      "   +- SortMergeJoin [product_id#17], [product_id#75], Inner\n",
      "      :- Sort [product_id#17 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(product_id#17, 200), ENSURE_REQUIREMENTS, [id=#259]\n",
      "      :     +- Filter isnotnull(product_id#17)\n",
      "      :        +- FileScan csv [order_id#16,product_id#17,customer_id#18,quantity#19,order_date#20,total_amount#21] Batched: false, DataFilters: [isnotnull(product_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/data/bucke..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<order_id:int,product_id:int,customer_id:int,quantity:int,order_date:string,total_amount:int>\n",
      "      +- Sort [product_id#75 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(product_id#75, 200), ENSURE_REQUIREMENTS, [id=#260]\n",
      "            +- Filter isnotnull(product_id#75)\n",
      "               +- FileScan csv [product_id#75,product_name#76,category#77,brand#78,price#79,stock#80] Batched: false, DataFilters: [isnotnull(product_id#75)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/data/bucke..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,product_name:string,category:string,brand:string,price:int,stock:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_product_details.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_product_details.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_products\n",
    "    .write.bucketBy(4, col=\"product_id\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"products_bucketed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_orders\n",
    "    .write.bucketBy(4, col=\"product_id\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"orders_bucketed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_bucketed = spark.table(\"orders_bucketed\")\n",
    "df_products_bucketed = spark.table(\"products_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_product_details_bucketed = (\n",
    "    df_orders_bucketed.join(\n",
    "        df_products_bucketed,\n",
    "        on=\"product_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [product_id#194, order_id#193, customer_id#195, quantity#196, order_date#197, total_amount#198, product_name#206, category#207, brand#208, price#209, stock#210]\n",
      "   +- SortMergeJoin [product_id#194], [product_id#205], Inner\n",
      "      :- Sort [product_id#194 ASC NULLS FIRST], false, 0\n",
      "      :  +- Filter isnotnull(product_id#194)\n",
      "      :     +- FileScan parquet default.orders_bucketed[order_id#193,product_id#194,customer_id#195,quantity#196,order_date#197,total_amount#198] Batched: true, DataFilters: [isnotnull(product_id#194)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/spark/spar..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<order_id:int,product_id:int,customer_id:int,quantity:int,order_date:string,total_amount:int>, SelectedBucketsCount: 4 out of 4\n",
      "      +- Sort [product_id#205 ASC NULLS FIRST], false, 0\n",
      "         +- Filter isnotnull(product_id#205)\n",
      "            +- FileScan parquet default.products_bucketed[product_id#205,product_name#206,category#207,brand#208,price#209,stock#210] Batched: true, DataFilters: [isnotnull(product_id#205)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/spark/spar..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,product_name:string,category:string,brand:string,price:int,stock:int>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_product_details_bucketed.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_product_details_bucketed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing In Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+--------+----------+------------+\n",
      "|order_id|product_id|customer_id|quantity|order_date|total_amount|\n",
      "+--------+----------+-----------+--------+----------+------------+\n",
      "|1       |80        |10         |4       |2023-3-20 |1003        |\n",
      "|2       |69        |30         |3       |2023-12-11|780         |\n",
      "|3       |61        |20         |4       |2023-4-26 |1218        |\n",
      "|4       |62        |44         |3       |2023-8-26 |2022        |\n",
      "|5       |78        |46         |4       |2023-8-5  |1291        |\n",
      "+--------+----------+-----------+--------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#17], functions=[sum(total_amount#21)])\n",
      "   +- Exchange hashpartitioning(product_id#17, 200), ENSURE_REQUIREMENTS, [id=#763]\n",
      "      +- HashAggregate(keys=[product_id#17], functions=[partial_sum(total_amount#21)])\n",
      "         +- FileScan csv [product_id#17,total_amount#21] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/data/bucke..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,total_amount:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WITHOUT BUCKETING\n",
    "\n",
    "df_product_sales = (\n",
    "    df_orders\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(F.sum(\"total_amount\").alias(\"sales\"))\n",
    ")\n",
    "\n",
    "df_product_sales.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#194], functions=[sum(total_amount#198)])\n",
      "   +- HashAggregate(keys=[product_id#194], functions=[partial_sum(total_amount#198)])\n",
      "      +- FileScan parquet default.orders_bucketed[product_id#194,total_amount#198] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/spark/spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,total_amount:int>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WITH BUCKETING\n",
    "\n",
    "df_product_sales = (\n",
    "    df_orders_bucketed\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(F.sum(\"total_amount\").alias(\"sales\"))\n",
    ")\n",
    "\n",
    "df_product_sales.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_sales_bucket_pruning = (\n",
    "    df_orders_bucketed\n",
    "    .filter(F.col(\"product_id\") == 1)\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(F.sum(\"total_amount\").alias(\"sales\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#194], functions=[sum(total_amount#198)])\n",
      "   +- HashAggregate(keys=[product_id#194], functions=[partial_sum(total_amount#198)])\n",
      "      +- Filter (isnotnull(product_id#194) AND (product_id#194 = 1))\n",
      "         +- FileScan parquet default.orders_bucketed[product_id#194,total_amount#198] Batched: true, DataFilters: [isnotnull(product_id#194), (product_id#194 = 1)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/spark/spar..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id), EqualTo(product_id,1)], ReadSchema: struct<product_id:int,total_amount:int>, SelectedBucketsCount: 1 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_product_sales_bucket_pruning.explain()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-skew-explanation",
   "notebookOrigID": 4018749166498458,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
