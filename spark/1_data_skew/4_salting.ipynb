{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12fa052b-65ab-42fe-92e9-866a91ab0479",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h2> Imports & Configuration </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/09 21:34:24 WARN Utils: Your hostname, Afaques-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.4 instead (on interface en0)\n",
      "23/07/09 21:34:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/09 21:34:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"3\")\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25578ece-41dd-49fa-807e-7b5798d875d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h2> Simulating Skewed Join </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2134bcf-2fd7-41c9-9fc1-32328c196482",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|0    |\n",
      "|1    |\n",
      "|2    |\n",
      "|3    |\n",
      "|4    |\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_uniform = spark.createDataFrame([i for i in range(1000000)], IntegerType())\n",
    "df_uniform.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a6b2e9-3543-4f5d-82ee-3f7e33139318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:====>                                                    (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|partition|count|\n",
      "+---------+-----+\n",
      "|0        |82944|\n",
      "|1        |82944|\n",
      "|2        |83968|\n",
      "|3        |82944|\n",
      "|4        |83968|\n",
      "|5        |82944|\n",
      "|6        |82944|\n",
      "|7        |83968|\n",
      "|8        |82944|\n",
      "|9        |83968|\n",
      "|10       |82944|\n",
      "|11       |83520|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_uniform\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"partition\")\n",
    "    .count()\n",
    "    .orderBy(\"partition\")\n",
    "    .show(15, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0babff23-9b27-439a-80b2-a424dc6733f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                (0 + 12) / 12][Stage 6:>                 (0 + 0) / 12]\r",
      "\r",
      "[Stage 6:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|0    |\n",
      "|0    |\n",
      "|0    |\n",
      "|0    |\n",
      "|0    |\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df0 = spark.createDataFrame([0] * 999990, IntegerType()).repartition(1)\n",
    "df1 = spark.createDataFrame([1] * 15, IntegerType()).repartition(1)\n",
    "df2 = spark.createDataFrame([2] * 10, IntegerType()).repartition(1)\n",
    "df3 = spark.createDataFrame([3] * 5, IntegerType()).repartition(1)\n",
    "df_skew = df0.union(df1).union(df2).union(df3)\n",
    "df_skew.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a33df63-459d-4bb3-88d2-5189334be1b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|partition| count|\n",
      "+---------+------+\n",
      "|        0|999990|\n",
      "|        1|    15|\n",
      "|        2|    10|\n",
      "|        3|     5|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_skew\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"partition\")\n",
    "    .count()\n",
    "    .orderBy(\"partition\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75213785-809b-492c-a0ff-5a775c36bfd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined_c1 = df_skew.join(df_uniform, \"value\", 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60ef213-f51c-4178-af15-92e7fc020bf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|partition|count  |\n",
      "+---------+-------+\n",
      "|0        |1000005|\n",
      "|1        |15     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:======================================>                   (2 + 1) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_joined_c1\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"partition\")\n",
    "    .count()\n",
    "    .show(5, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5156d1a2-2f8b-4391-af89-bfb119fd5dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h2> Simulating Uniform Distribution Through Salting </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f90516c-fd1d-431e-ab98-f8030377d4f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SALT_NUMBER = int(spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "SALT_NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8ee313-e419-425f-9e44-dc9ad0370d8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_skew = df_skew.withColumn(\"salt\", (F.rand() * SALT_NUMBER).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd3dbd79-a96d-4b0d-a97e-245d401b438a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|value|salt|\n",
      "+-----+----+\n",
      "|0    |2   |\n",
      "|0    |0   |\n",
      "|0    |0   |\n",
      "|0    |1   |\n",
      "|0    |2   |\n",
      "|0    |2   |\n",
      "|0    |0   |\n",
      "|0    |2   |\n",
      "|0    |2   |\n",
      "|0    |1   |\n",
      "+-----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_skew.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "455a8ccf-6f83-467f-b4ef-e49e98c604bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_uniform = (\n",
    "    df_uniform\n",
    "    .withColumn(\"salt_values\", F.array([F.lit(i) for i in range(SALT_NUMBER)]))\n",
    "    .withColumn(\"salt\", F.explode(F.col(\"salt_values\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f802a70d-158e-496b-8e9f-b2ce415c5ca7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----+\n",
      "|value|salt_values|salt|\n",
      "+-----+-----------+----+\n",
      "|0    |[0, 1, 2]  |0   |\n",
      "|0    |[0, 1, 2]  |1   |\n",
      "|0    |[0, 1, 2]  |2   |\n",
      "|1    |[0, 1, 2]  |0   |\n",
      "|1    |[0, 1, 2]  |1   |\n",
      "|1    |[0, 1, 2]  |2   |\n",
      "|2    |[0, 1, 2]  |0   |\n",
      "|2    |[0, 1, 2]  |1   |\n",
      "|2    |[0, 1, 2]  |2   |\n",
      "|3    |[0, 1, 2]  |0   |\n",
      "+-----+-----------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_uniform.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75206f86-10da-4d84-8bdf-fb825cbeaab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined = df_skew.join(df_uniform, [\"value\", \"salt\"], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd237984-e909-48b4-8f45-ffedb6541447",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------+\n",
      "|value|partition| count|\n",
      "+-----+---------+------+\n",
      "|    0|        0|332774|\n",
      "|    0|        1|333601|\n",
      "|    0|        2|333615|\n",
      "|    1|        0|     6|\n",
      "|    1|        1|     9|\n",
      "|    2|        0|     2|\n",
      "|    2|        1|     2|\n",
      "|    2|        2|     6|\n",
      "|    3|        0|     3|\n",
      "|    3|        1|     2|\n",
      "+-----+---------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_joined\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"value\", \"partition\")\n",
    "    .count()\n",
    "    .orderBy(\"value\", \"partition\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3dec6ed-daf6-4615-be71-efb313cb9a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Salting In Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|value| count|\n",
      "+-----+------+\n",
      "|    0|999990|\n",
      "|    2|    10|\n",
      "|    3|     5|\n",
      "|    1|    15|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_skew.groupBy(\"value\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_skew\n",
    "    .withColumn(\"salt\", (F.rand() * SALT_NUMBER).cast(\"int\"))\n",
    "    .groupBy(\"value\", \"salt\")\n",
    "    .agg(F.count(\"value\").alias(\"count\"))\n",
    "    .groupBy(\"value\")\n",
    "    .agg(F.sum(\"count\").alias(\"count\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-skew-explanation",
   "notebookOrigID": 4018749166498458,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
