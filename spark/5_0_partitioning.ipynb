{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/14 16:46:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"6_0_partitioning\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------------------------+---------------+\n",
      "|activity_id|song_id|listen_date               |listen_duration|\n",
      "+-----------+-------+--------------------------+---------------+\n",
      "|1          |12     |2023-06-27 10:15:47.008867|69             |\n",
      "|2          |44     |2023-06-27 10:15:47.008867|300            |\n",
      "|3          |75     |2023-06-27 10:15:47.008867|73             |\n",
      "|4          |48     |2023-06-27 10:15:47.008867|105            |\n",
      "|5          |10     |2023-06-27 10:15:47.008867|229            |\n",
      "+-----------+-------+--------------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listening_activity_file = \"../data/partitioning/raw/Spotify_Listening_Activity.csv\"\n",
    "df_listening_actv = spark.read.csv(listening_activity_file, header=True, inferSchema=True)\n",
    "df_listening_actv.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------------------------+---------------+-----------+-----------+\n",
      "|activity_id|song_id|listen_time               |listen_duration|listen_date|listen_hour|\n",
      "+-----------+-------+--------------------------+---------------+-----------+-----------+\n",
      "|1          |12     |2023-06-27 10:15:47.008867|69             |2023-06-27 |10         |\n",
      "|2          |44     |2023-06-27 10:15:47.008867|300            |2023-06-27 |10         |\n",
      "|3          |75     |2023-06-27 10:15:47.008867|73             |2023-06-27 |10         |\n",
      "|4          |48     |2023-06-27 10:15:47.008867|105            |2023-06-27 |10         |\n",
      "|5          |10     |2023-06-27 10:15:47.008867|229            |2023-06-27 |10         |\n",
      "+-----------+-------+--------------------------+---------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- activity_id: integer (nullable = true)\n",
      " |-- song_id: integer (nullable = true)\n",
      " |-- listen_time: string (nullable = true)\n",
      " |-- listen_duration: integer (nullable = true)\n",
      " |-- listen_date: date (nullable = true)\n",
      " |-- listen_hour: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11779"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_listening_actv = (\n",
    "    df_listening_actv\n",
    "    .withColumnRenamed(\"listen_date\", \"listen_time\")\n",
    "    .withColumn(\"listen_date\", F.to_date(\"listen_time\", \"yyyy-MM-dd HH:mm:ss.SSSSSS\"))\n",
    "    .withColumn(\"listen_hour\", F.hour(\"listen_time\"))\n",
    ")\n",
    "\n",
    "df_listening_actv.show(5, False)\n",
    "df_listening_actv.printSchema()\n",
    "df_listening_actv.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning By `listen_date`\n",
    "\n",
    "Let's say we want to **analyse the listening behaviours of user over time**. If we're given the complete dataset (with no partitions), Spark would scan the whole dataset for finding a particular date (similar to the bookshelf analogy where you would scan the entire bookself for finding a book if it is not organized). Given that our usecase needs analysis by date, partitioning (creating folders) on date would help Spark pin point to the exact folder. This makes searching very easy and Spark doesn't scan the entire dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Partitioning listening activity by the listen date\n",
    "(\n",
    "    df_listening_actv\n",
    "    .write\n",
    "    .partitionBy(\"listen_date\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"../data/partitioning/partitioned/listening_activity_pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [activity_id#250,song_id#251,listen_time#252,listen_duration#253,listen_hour#254,listen_date#255] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/afaqueahmad/Documents/YouTube/spark-experiments/data/parti..., PartitionFilters: [isnotnull(listen_date#255), (listen_date#255 = 2019-01-01)], PushedFilters: [], ReadSchema: struct<activity_id:int,song_id:int,listen_time:string,listen_duration:int,listen_hour:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_listening_actv_pt_pruned = spark.read.parquet(\"../data/partitioning/partitioned/listening_activity_pt\")\n",
    "df_listening_actv_pt_pruned.filter(\"listen_date = '2019-01-01'\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Problems Does Partitioning Solve? \n",
    "1. `Fast Search (Query Performance)`: Spark will only process the relevant partition instead of the entire dataset (example above). This greatly reduces I/O and query execution time. \n",
    "2. `Parallelism / Resource Utilization`: Each core processes 1 partition; More number of partitions, more is the parallelism; again this does not mean we forcefully increase the number of partitions. Each partition should be `128MB` in size. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Examples\n",
    "1. Single/multi level partitioning\n",
    "2. Using `repartition`/`coalesce` with `partitionBy` (controlling number of files inside each partition): \n",
    "    - `parititionBy` affects how data is laid out in the storage and is going to ensure that the output directory is organized into subdirectories based on the `value` given in `partitionBy`.  \n",
    "    - Number of files in each `value` directory of `partitionBy` depends on the number supplied in the `repartition`/`coalesce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Single/multi level partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_listening_actv\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"listen_date\", \"listen_hour\")\n",
    "    .parquet(\"../data/partitioning/partitioned/listening_activity_pt_2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_listening_actv\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"listen_hour\", \"listen_date\")\n",
    "    .parquet(\"../data/partitioning/partitioned/listening_activity_pt_3\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using `repartition`/`coalesce` with `partitionBy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_listening_actv\n",
    "    .repartition(3)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"listen_date\")\n",
    "    .parquet(\"../data/partitioning/partitioned/listening_activity_pt_4\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coalesce method reduces the number of partitions in a DataFrame. \n",
    "# It avoids full shuffle, instead of creating new partitions, it shuffles the data using default Hash Partitioner, \n",
    "# and adjusts into existing partitions, this means it can only decrease the number of partitions.\n",
    "\n",
    "(\n",
    "    df_listening_actv\n",
    "    .coalesce(3)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"listen_date\")\n",
    "    .parquet(\"../data/partitioning/partitioned/listening_activity_pt_5\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting With `spark.sql.files.maxPartitionBytes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "spark = SparkSession.builder.appName(\"Test spark.sql.files.maxPartitionBytes\").getOrCreate()\n",
    "\n",
    "df_default = spark.read.csv(\"../data/partitioning/raw/listening_activity.csv\", header=True, inferSchema=True)\n",
    "default_partitions = df_default.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions with default maxPartitionBytes: {default_partitions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"1000\")\n",
    "\n",
    "df_modified = spark.read.csv(\"../data/partitioning/raw/listening_activity.csv\", header=True, inferSchema=True)\n",
    "modified_partitions = df_modified.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions with modified maxPartitionBytes: {modified_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-skew-explanation",
   "notebookOrigID": 4018749166498458,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
